import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime

id_list = [
    (1, 189, "一般女生組100公尺自由式 預賽"),(2, 149, "一般男生組50公尺蛙式 預賽"),(3, 111, "一般女生組50公尺背式 預賽"),(4, , ""),
    (5, 209, "一般女生組100公尺蝶式 預賽"),(6, , ""),(7, , ""),(8, , ""),(9, , ""),(10, , ""),
    (11, , ""),(12, , ""),(13, , ""),(14, , ""),(15, , ""),
    (16, , ""),(17, , ""),(18, , ""),(19, , ""),(20, , ""),
    (21, , ""),(22, , ""),(23, , ""),(24, 208, "一般女生組100公尺蝶式 決賽"),(25, , ""),
    (26, , ""),(27, , ""),(28, , ""),(29, , ""),(30, , ""),
    (31, , ""),(32, , ""),(33, , ""),(34, , ""),(35, , ""),
    (36, , ""),(37, , ""),(38, , ""),(39, , ""),(40, , ""),
    (41, , ""),(42, , ""),(43, , ""),(44, , ""),(45, 185, "一般女生組100公尺自由式 預賽"),
    (46, , ""),(47, , ""),(48, , ""),(49, , ""),(50, , ""),
    (51, , ""),(52, , ""),(53, , ""),(54, , ""),(55, , ""),
    (56, , ""),(57, , ""),(58, , ""),(59, , ""),(60, , ""),
    (61, , ""),(62, 184, "一般女生組100公尺自由式 決賽"),(63, , ""),(64, , ""),(65, , ""),
    (66, , ""),(67, , ""),(68, , ""),(69, , ""),(70, , ""),
    (71, , ""),(72, , ""),(73, , ""),(74, , ""),(75, , ""),
    (76, , ""),(77, , ""),(78, , ""),(79, 203, "一般女生組游泳 100公尺仰式 預賽"),(80, , ""),
    (81, , ""),(82, , ""),(83, 197, "一般女生組100公尺蛙式 預賽"),(84, , ""),(85, , ""),
    (86, , ""),(87, , ""),(88, , ""),(89, , ""),(90, , ""),
    (91, , ""),(92, , ""),(93, , ""),(94, , ""),(95, , ""),
    (96, , ""),(97, 202, "一般女生組游泳 100公尺仰式 決賽"),(98, , ""),(99, , ""),(100, , ""),
    (101, 196, "一般女生組100公尺蛙式 決賽"),(102, , ""),(103, , ""),(104, , ""),(105, , ""),
    (106, , ""),(107, , ""),(108, , ""),(109, 193, "一般女生組1500公尺自由式 慢組計時決賽"),(110, , ""),
    (11, , ""),(11, , ""),(11, , ""),(11, , ""),(11, , ""),
    (11, , ""),(11, , ""),(11, , ""),(11, , ""),(12, , ""),
    (12, , ""),(12, , ""),(12, , ""),(12, , ""),(12, , ""),
    (12, , ""),(12, , ""),(128, 192, "一般女生組1500公尺自由式 快組計時決賽"),(12, , ""),(13, , ""),
    (13, , ""),(13, , ""),(13, , ""),(13, , ""),(13, , ""),
    (13, , ""),(13, , ""),(13, , ""),(13, , ""),(14, , ""),
    (14, , ""),(14, , ""),(14, , ""),(14, , ""),(14, , ""),
    (14, , ""),(14, , ""),(14, , ""),(14, , ""),(15, , ""),
    (15, , ""),(15, , ""),(15, , ""),(15, , ""),(15, , ""),
    (15, , ""),(15, , ""),(15, , ""),(15, , ""),(16, , ""),
    (16, , ""),(16, , ""),(16, , ""),(16, , ""),(16, , ""),
    (16, , ""),(16, , ""),(16, , ""),(16, , ""),(17, , ""),
    (17, , ""),(17, , ""),(17, , ""),(17, , ""),(17, , ""),
    (17, , ""),(17, , ""),(17, , ""),(17, , ""),(18, , ""),
    (18, , ""),(18, , ""),(18, , ""),(18, , ""),(18, , ""),
    (18, , ""),(18, , ""),(18, , ""),(18, , ""),(19, , ""),
    (19, , ""),(19, , ""),(19, , ""),(19, , ""),(19, , ""),
    (19, , ""),(19, , ""),(19, , ""),(19, , ""),(20, , ""),
    (20, , ""),(20, , ""),(20, , ""),(20, , ""),(20, , ""),
    (20, , ""),(20, , ""),(20, , ""),(20, , ""),(21, , ""),
    (21, , ""),(21, , ""),(21, , ""),(21, , ""),(215, , ""),
    (216, , ""),
]

def fetch_score(id_):
    url = f"https://114niag.cjcu.edu.tw/Public/Race/Report_Score.aspx?id={id_}"
    print(f"Fetching {url}")
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.85 Safari/537.36"
        }
        res = requests.get(url, headers=headers, timeout=10, verify=False)  # <--- 這裡加 verify=False
        print(f"Status Code: {res.status_code}")
        res.raise_for_status()
    except requests.RequestException as e:
        print(f"Request error: {e}")
        return None

    soup = BeautifulSoup(res.text, 'html.parser')
    table = soup.find('table', id='table1')
    if not table:
        print("No table found.")
        return None

    rows = []
    for tr in table.find_all('tr'):
        cols = [td.get_text(strip=True) for td in tr.find_all(['td', 'th'])]
        if cols:
            rows.append(cols)

    if len(rows) < 2:
        print("Table has no data rows.")
        return None

    headers = rows[0]
    data_rows = rows[1:]

    records = []
    for row in data_rows:
        if len(row) >= len(headers):
            record = dict(zip(headers, row))
            records.append(record)

    return records

def main():
    result = []
    for order, id_, title in id_list:
        scores = fetch_score(id_)
        if scores:
            result.append({
                "order": order,
                "id": id_,
                "title": title,
                "timestamp": datetime.now().isoformat(),
                "scores": scores
            })

    with open('data/score.json', 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    main()
